{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, noleave and normal data would be used for training and testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "dataset=np.load('AllSetSparseInOut_noleave_N.npz')\n",
    "TrainX=np.asmatrix(dataset['TrainX'])[0,0]\n",
    "ValidX=np.asmatrix(dataset['ValidX'])[0,0]\n",
    "TestX=np.asmatrix(dataset['TestX'])[0,0]\n",
    "TrainY=np.asmatrix(dataset['TrainY'])[0,0]\n",
    "ValidY=np.asmatrix(dataset['ValidY'])[0,0]\n",
    "TestY=np.asmatrix(dataset['TestY'])[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "TrainY = np.array(TrainY.todense()).ravel()\n",
    "ValidY = np.array(ValidY.todense()).ravel()\n",
    "n_estimators = 20\n",
    "clf = BaggingClassifier(svm.SVC(kernel='poly',degree=2), max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=n_estimators)\n",
    "clf.fit(TrainX, TrainY)\n",
    "valid_score = clf.score(ValidX,ValidY) \n",
    "train_score = clf.score(TrainX,ValidY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation score: 0.5395456542996463  \n",
    "training score: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When trained with original dataset (with leavers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the partial training data (random 10,000) used: training accuracy over 2,000 is 0.657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When trained with noleave dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the partial training data (random 10,000) used: training accuracy over 2,000 is 0.653"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDA code is provided as below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "dataset=np.load('AllSetSparseInOut_noleave_N.npz')\n",
    "trainX=np.asmatrix(dataset['TrainX'])[0,0]\n",
    "validX=np.asmatrix(dataset['ValidX'])[0,0]\n",
    "testX=np.asmatrix(dataset['TestX'])[0,0]\n",
    "TrainY=np.asmatrix(dataset['TrainY'])[0,0]\n",
    "ValidY=np.asmatrix(dataset['ValidY'])[0,0]\n",
    "TestY=np.asmatrix(dataset['TestY'])[0,0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer data into proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = np.array(TrainY.todense()).ravel()\n",
    "validY = np.array(ValidY.todense()).ravel()\n",
    "testY = np.array(TestY.todense()).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute mean and coverances for GDA and calculate likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_mles(train_data, train_labels):\n",
    "    '''\n",
    "    Compute the mean estimate for each digit class\n",
    "\n",
    "    Should return a numpy array of size (10,64)\n",
    "    The ith row will correspond to the mean estimate for digit class i\n",
    "    '''\n",
    "    means = np.zeros((2, train_data.shape[1]))\n",
    "    # Compute means\n",
    "    for i in range(means.shape[0]): # loop for each class\n",
    "        means[i] = np.mean(train_data[train_labels==i],axis=0)\n",
    "    return means\n",
    "\n",
    "def compute_sigma_mles(train_data, train_labels):\n",
    "    '''\n",
    "    Compute the covariance estimate for each digit class\n",
    "\n",
    "    Should return a three dimensional numpy array of shape (10, 64, 64)\n",
    "    consisting of a covariance matrix for each digit class \n",
    "    '''\n",
    "    covariances = np.zeros((2, train_data.shape[1], train_data.shape[1]))\n",
    "    # Compute covariances\n",
    "    means = compute_mean_mles(train_data, train_labels)\n",
    "    for i in range(covariances.shape[0]): # loop for each class\n",
    "        trainX = train_data[train_labels==i]\n",
    "        expe = trainX - means[i]\n",
    "        covariances[i] = 1 / trainX.shape[0] * np.matmul(expe.T, expe) + 0.01*np.identity(100)\n",
    "    return covariances\n",
    "\n",
    "def generative_likelihood(digits, means, covariances):\n",
    "    '''\n",
    "    Compute the generative log-likelihood:\n",
    "        log p(x|y,mu,Sigma)\n",
    "\n",
    "    Should return an n x 10 numpy array \n",
    "    '''\n",
    "    classlikelihood = np.zeros((digits.shape[0],2))\n",
    "    n = digits.shape[0]\n",
    "    d = digits.shape[1]\n",
    "    for i in range(n):\n",
    "        for j in range(2):\n",
    "            classlikelihood[i,j] = -d/2 * np.log(2*np.pi) - 1/2*np.log(np.linalg.det(covariances[j])) - 1/2*np.matmul(\n",
    "                np.matmul(\n",
    "                    digits[i]-means[j],np.linalg.inv(covariances[j])\n",
    "                    )\n",
    "                ,(digits[i]-means[j]).T\n",
    "                    )\n",
    "        print('class likelihood for the %dth data is %s' % (i,classlikelihood[i]))\n",
    "    return classlikelihood\n",
    "\n",
    "def conditional_likelihood(digits, means, covariances):\n",
    "    '''\n",
    "    Compute the conditional likelihood:\n",
    "\n",
    "        log p(y|x, mu, Sigma)\n",
    "\n",
    "    This should be a numpy array of shape (n, 10)\n",
    "    Where n is the number of datapoints and 10 corresponds to each digit class\n",
    "    '''\n",
    "    classlikelihood = generative_likelihood(digits, means, covariances)\n",
    "    posterior = 1/2*classlikelihood\n",
    "    return posterior\n",
    "\n",
    "\n",
    "def classify_data(digits, means, covariances):\n",
    "    '''\n",
    "    Classify new points by taking the most likely posterior class\n",
    "    '''\n",
    "    cond_likelihood = conditional_likelihood(digits, means, covariances)\n",
    "    # Compute and return the most likely class\n",
    "    res = [np.argmax(cond_likelihood[i]) for i in range(cond_likelihood.shape[0])]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Prepare subset of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000\n",
    "indices = random.sample(range(TrainX.shape[0]), n_samples)\n",
    "trainX_sub = TrainX[indices]\n",
    "trainY_sub = trainY[indices]\n",
    "\n",
    "n_samples = 100000\n",
    "indices = random.sample(range(ValidX.shape[0]), n_samples)\n",
    "validX_sub = ValidX[indices]\n",
    "validY_sub = validY[indices]\n",
    "\n",
    "n_samples = 100000\n",
    "indices = random.sample(range(TestX.shape[0]), n_samples)\n",
    "testX_sub = TestX[indices]\n",
    "testY_sub = testY[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Firstly compute the mean and covariances given training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = trainX\n",
    "train_labels = trainY\n",
    "means = compute_mean_mles(train_data, train_labels)\n",
    "covariances = compute_sigma_mles(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then compute it by calling classify_data() and calculate accuracy thereafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "indices = random.sample(range(trainX_sub.shape[0]), n_samples)\n",
    "res_train = classify_data(trainX_sub[indices], means, covariances)\n",
    "train_acc = sum(res_train==trainY_sub[indices])/len(res_train) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
