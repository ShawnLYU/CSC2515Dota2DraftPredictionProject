{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "Load training, testing, and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX=sparse.load_npz('../data/trainsetInputVector_sparse.npz')\n",
    "trainY = np.genfromtxt('../data/trainsetResult.csv', delimiter='\\n')\n",
    "\n",
    "validX=sparse.load_npz('../data/validsetInputVector_sparse.npz')\n",
    "validY = np.genfromtxt('../data/validsetResult.csv', delimiter='\\n')\n",
    "\n",
    "testX=sparse.load_npz('../data/testsetInputVector_sparse.npz')\n",
    "testY = np.genfromtxt('../data/testsetResult.csv', delimiter='\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use [sklearn.ensemble.BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) here to speed up the training process while keep using the whole data set, which will split the dataset and train on each of them, and finally using the average classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging of SVM with linear kernel to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 20\n",
    "clf = BaggingClassifier(svm.LinearSVC(), max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=n_estimators)\n",
    "clf.fit(trainX, trainY)\n",
    "clf.score(validX,validY) # valid_score = 0.6235049863363736\n",
    "clf.score(validX_sub,validY_sub) # valid_score = 0.6213"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging of SVM with sigmoid kernel to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_estimators = 20\n",
    "clf = BaggingClassifier(svm.SVC(kernel='sigmoid',), max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=n_estimators)\n",
    "clf.fit(trainX, trainY)\n",
    "valid_score = clf.score(validX,validY) # valid_score = 0.6235049863363736"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging of SVM with poly kernel to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 20\n",
    "clf = BaggingClassifier(svm.SVC(kernel='poly',), max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=n_estimators)\n",
    "clf.fit(trainX, trainY)\n",
    "valid_score = clf.score(validX_sub,validY_sub) # valid_score = 0.53057"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to speed up the process is to randomly select a subset of the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select n_samples and targets accordingly\n",
    "n_samples = 100000\n",
    "indices = random.sample(range(trainX.shape[0]), n_samples)\n",
    "trainX_sub = trainX[indices]\n",
    "trainY_sub = trainY[indices]\n",
    "\n",
    "n_samples = 100000\n",
    "indices = random.sample(range(validX.shape[0]), n_samples)\n",
    "validX_sub = validX[indices]\n",
    "validY_sub = validY[indices]\n",
    "\n",
    "n_samples = 100000\n",
    "indices = random.sample(range(testX.shape[0]), n_samples)\n",
    "testX_sub = testX[indices]\n",
    "testY_sub = testY[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with linear kernel with random sampled a subset\n",
    "\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(trainX_sub, trainY_sub)\n",
    "valid_score = clf.score(validX_sub,validY_sub) # valid_score = 0.62153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with linear kernel with random sampled a subset with different C\n",
    "\n",
    "res = [] \n",
    "for c in np.arange(1,100,10):\n",
    "    print('C=%d' % c)\n",
    "    clf = svm.LinearSVC(C=c)\n",
    "    clf.fit(trainX_sub, trainY_sub)\n",
    "    res.append(clf.score(validX_sub,validY_sub))\n",
    "plt.plot(np.arange(1,100,10),res)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Penalty the error term')\n",
    "plt.savefig('svm_pics/svm_poly_diff_Cs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with poly with random sampled a subset\n",
    "n_estimators = 10\n",
    "clf = BaggingClassifier(svm.SVC(kernel='poly',), max_samples=1.0 / n_estimators, n_estimators=n_estimators,n_jobs=n_estimators)\n",
    "clf.score(validX_sub,validY_sub) # valid_score = 0.53018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LogisticRegression supports n_jobs, therefore BaggingClassifier is left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried different penalty term\n",
    "scores = []\n",
    "for C in [1,10,1e3,1e4,1e5]:\n",
    "    print('Processing %d' % C)\n",
    "    logreg = LogisticRegression(C=C, solver='lbfgs', n_jobs=20)\n",
    "    logreg.fit(trainX, trainY)\n",
    "    scores.append(logreg.score(validX_sub,validY_sub))\n",
    "\n",
    "'''\n",
    "Outputs:\n",
    "scores\n",
    "[0.62503, 0.62502, 0.62501, 0.62501, 0.62501]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using L1 penalty\n",
    "logreg = LogisticRegression(C=1e5, solver='saga', penalty='l1', n_jobs=20)\n",
    "logreg.fit(trainX, trainY)\n",
    "logreg.score(validX_sub,validY_sub) # score = 0.62176"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "trainX_transformed = pca.fit_transform(trainX.todense())\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "trainX_normalized = scaler.fit_transform(trainX.todense())\n",
    "trainX_transformed_normalized = scaler.fit_transform(trainX_transformed)\n",
    "\n",
    "import seaborn as sns\n",
    "for i in range(10):\n",
    "    print('processing %d' % i)\n",
    "    plt.clf()\n",
    "    sns.distplot(trainX_transformed_normalized[trainY==0][:,i], rug=True) \n",
    "    plt.savefig('svm_pics/pc%d_0.png' % i)\n",
    "    plt.clf()\n",
    "    sns.distplot(trainX_transformed_normalized[trainY==1][:,i], rug=True)\n",
    "    plt.savefig('svm_pics/pc%d_1.png' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
